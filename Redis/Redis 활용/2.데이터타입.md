
## 데이터 타입

### Strings

* 문자열, 숫자, serialized object(JSON string) 데이터를 저장할 수 있다.
* SET 명령어로 데이터 저장
* MSET 명령어로 여러 데이터 저장
* MGET 명령어로 여러 데이터 조회
* 별도의 integer 타입이 없다. 모두 문자열로 저장된다.
* INCR 명령어로 숫자형 스트링을 증가시킬 수 있다.
* INCRBY 명령어로 숫자형 스트링값에 특정 숫자를 더할 수 있다.
* JSON object 값을 저장할 수 있다.
* 콜론(:)으로 키를 만들때 의미별로 구분한다.

### Lists

* string 을 linked list 로 저장 -> push / pop 에 최적화 O(1)
* Queue, Stack 구현 가능
* lpush : 리스트의 왼쪽에 데이터 추가
* rpop : 리스트의 오른쪽에서 데이터를 가져옴
* lpush , rpop 을 이용하여 Queue 구현
* lpush , lpop 을 이용하여 Stack 구현
* lrange : 리스트의 범위를 지정하여 데이터 조회(인덱스 0 부터 시작, 오른쪽 인덱스는 -1부터 시작)
* ltrim : 나머지 목록 삭제

### sets

* unique string 을 저장하는 정렬되지 않은 집합
* set operation 사용가능(e.g. intersection, union, difference)
* sadd : set 에 데이터 추가
* smembers : set 에 저장된 데이터 조회
* scard : set 에 저장된 고유한 아이템 개수를 출력
* sismember : set 에 특정 데이터가 존재하는지 여부를 확인
* sinter : 여러 set 의 교집합을 구함
* sdiff : 여러 set 의 차집합을 구함
* sunion : 여러 set 의 합집합을 구함

### hashes

* field-value 구조를 갖는 데이터 타입
* 다양한 속성을 갖는 객체의 데이터를 저장할때 유용
* hset : hash 에 데이터 추가
* hget : hash 에 저장된 데이터 조회
* hmget : hash 에 저장된 여러 데이터 조회
* hincrby : hash 에 저장된 숫자형 데이터를 증가시킴

### sorted sets

* Unique string 을 연관된 score 를 통해 정렬된 집합(set + score)저장
* 내부적으로 skip list + hash table 로 이루어져 있고 score 값에 따라 정렬 유지
* score 가 동일 하면 사전 편찬순 정(lexicographically)
* zset 이라고 불림
* zadd : sorted set 에 데이터 추가
* zrange : sorted set 에 저장된 데이터 조회
* zrank : sorted set 에 저장된 데이터의 순위를 조회(인덱스 값과 동일)

### strams

* append-only log 에 consumer groups 과 같은 기능을 더한 자료 구종
* unique id 를 통해 하나의 entry 를 읽을때 O(1) 시간 복잡도
* consumer group 을 통해 분산 시스템에서 다수의 consumer 가 event 처리
* xadd : stream 에 데이터 추가
* arange : stream 에 저장된 데이터 조회
* xdel : stream 에 저장된 데이터 삭제

### geospatioals

* 좌표를 저장하거 검색하는 데이터 타입
* 거리계산, 범위 탐색등 지원
* geoadd : geospatial 에 데이터 추가
* geodist : geospatial 에 저장된 데이터의 거리를 계산

### bitmaps

* 실제 데이터 타입은 아니고 string 에 binary operation 을 적용한것
* 최대 42억개 binary 데이터 표현 2^32(4,294,967,296)
* setbit : bitmap 에 데이터 추가
* bitcount : bitmap 에 저장된 데이터의 개수를 조회
* bitop and : bitmap 에 저장된 데이터의 교집합을 구함
* getbit : bitmap 에 저장된 데이터를 조회

### hyperloglog

* 집합의 cardinality 를 추정하는 확률적 자료구조
* 정확성을 일부 포기하는 대신 저장공간을 효율적으로 사용(에러0.81%)
* vs set : cardinality 를 정확하게 알 수 있지만 저장공간이 많이 필요
* pfadd : hyperloglog 에 데이터 추가

### bloomfilter

* elements 가 집합 안에 포함되었는지 확인할수 있는 확률형 자료구조
* 정확성을 일부 포기하는 대신 저장공간을 효율적으로 사용
* element 가 집합에 실제로 포함되지 않는데 포함되어있다고 잘못 예측하는 경우
* vs set : cardinality 를 정확하게 알 수 있지만 저장공간이 많이 필요
* bf.madd : bloomfilter 에 데이터 추가
* bf.exists : bloomfilter 에 데이터가 존재하는지 여부를 확인

### 데이터 만료

* expiration 을 통해 특정 시간 이후 만료시키는 기능
* TTL(time to live) : 데이터가 만료되기까지 남은 시간 초단위로 표현
* 데이터 조회 요청시 만료된 데이터는조회되지 않음
* 데이터가 만료되면 삭제하지 않고 만료로 표시하고 백그라운드에서 주기적으로 삭제
* expire : 데이터 만료시간 설정
* setex : 데이터 저장과 동시에 만료시간 설정

### NX/XX

* NX : 해당 key 가 존재하지 않을때만 데이터 저장
* XX : 해당 key 가 존재할때만 데이터 저장
* set 이 동작하지 않는 경우 nil 응답을 반환

### pub/sub

* publish/subscribe 가 서로 알지 못해도 통신이 가능하도록 decoupleing 된 패턴
* publisher 는 subscriber 에게 직접 메시지를 보내지 않고 channel 을 통해 메시지를 보냄
* subscriber 는 관심있는 channel 을 필요에 따라 구독하고 메시지를 받음
* vs steam : 메시지가 보관되는 stram 과 달리 pub/sub 은 subscibe 하지 않을때 발행된 메시지 수신 불가
* subscribe ch : channel 을 구독
* publish ch msg : channel 에 메시지를 발행

### pipelone

* 다수의 commands 를 한번에 요청하여 네트워크 성능을 향상 시키는 기술
* Round-Trip Times 최소화
* 대부분의 클라이언트 라이브러리에서 지원
* Round-Trip Times :  Request / Response 모델에서 발생하는 네트워크 지연시간


### transaction

* 다수의 명령을 하나의 트랜잭션으로 처리 -> atomicity 보장(원자성)
* 중간에 에러가 발생하면 모든 작업 rollback
* 하나의 트랜잭션이 처리되는 동안 다른 클라이언트의 요청이 중간에 끼어들수 없음
* all or noting : 모든 작업이 성공하거나 실패
* pipe : pipeline 을 사용하여 여러개의 명령어를 한번에 요청
* pipeline 은 네트워크 퍼포먼스 향상을 위해 여러개의 명령어를 한번에 요청
* transcation 은 작업의 원자성을 보장하기 위해 다수의 명령어를 하나처럼 처리하는 기술
* multi : 트랜잭션 시작
* discard : 트랜잭션 롤백
* exec : 트랜잭션 커밋

